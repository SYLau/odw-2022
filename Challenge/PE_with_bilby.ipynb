{"metadata":{"colab":{"name":"Tuto_3.2_Parameter_estimation_for_compact_object_mergers.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Initialization","metadata":{"Collapsed":"false","colab_type":"text","id":"XK8fHu13ygu1"}},{"cell_type":"code","source":"#! pip install -q lalsuite\n#! pip install -q gwpy\n#! pip install -q pycbc\n#! pip install -q lalsuite==6.82 bilby==1.0.4 gwpy==2.0.2 matplotlib==3.2.2 dynesty==1.0.0\n\n# -- Click \"restart runtime\" in the runtime menu","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nfrom __future__ import division, print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport bilby\nfrom bilby.core.prior import Uniform\nfrom bilby.gw.conversion import convert_to_lal_binary_black_hole_parameters, generate_all_bbh_parameters\n\nfrom gwpy.timeseries import TimeSeries\n\n# -- download data\n! wget https://www.gw-openscience.org/s/workshop3/challenge/challenge3.gwf\n\nprint(bilby.__version__)","metadata":{"Collapsed":"false","colab":{},"colab_type":"code","id":"HyRSGt6cygu2","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2022-05-29 23:47:09--  https://www.gw-openscience.org/s/workshop3/challenge/challenge3.gwf\nResolving www.gw-openscience.org (www.gw-openscience.org)... 131.215.113.73\nConnecting to www.gw-openscience.org (www.gw-openscience.org)|131.215.113.73|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 256815066 (245M)\nSaving to: ‘challenge3.gwf’\n\nchallenge3.gwf      100%[===================>] 244.92M  7.46MB/s    in 37s     \n\n2022-05-29 23:47:47 (6.70 MB/s) - ‘challenge3.gwf’ saved [256815066/256815066]\n\n1.1.5: (CLEAN) 88cf7e2 2022-01-24 22:26:41 +0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Read data","metadata":{}},{"cell_type":"code","source":"#sampling_rate=2048 #needs to be high enough for the signals found in steps above\nsampling_rate=4096 #needs to be high enough for the signals found in steps above\n\nparameter_set = 5\nif parameter_set == 1:\n    # 10 M_sun\n    label_mass_from_mf = '10'\n    mass_from_mf_min = 8.5\n    mass_from_mf_max = 12.5\n    duration=2.4 #needs to be long enough for the signals found in steps above\n    start_time=2481.8 #needs to be set so that the segment defined by [start_time,start_time+duration] contains the signal\nelif parameter_set == 2:\n    # 17 M_sun\n    mass_from_mf_min = 14\n    mass_from_mf_max = 19\n    label_mass_from_mf = '17'\n    duration=4\n    start_time=1637\nelif parameter_set == 3:\n    # 22 M_sun\n    mass_from_mf_min = 17\n    mass_from_mf_max = 28\n    label_mass_from_mf = '22'\n    duration=1\n    start_time=1203.8\nelif parameter_set == 4:\n    # 31 M_sun\n    mass_from_mf_min = 26\n    mass_from_mf_max = 35\n    label_mass_from_mf = '31'\n    duration=0.8\n    start_time=2994.8\nelif parameter_set == 5:\n    # 35 M_sun\n    mass_from_mf_min = 31\n    mass_from_mf_max = 40\n    label_mass_from_mf = '35'\n    duration=0.5\n    start_time=3318.9","metadata":{"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    else if parameter_set == 2:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2309088207.py, line 12)","output_type":"error"}]},{"cell_type":"code","source":"psd_duration = duration * 32\npsd_start_time = start_time - psd_duration\n\ninterferometers = bilby.gw.detector.InterferometerList([])\nfor ifo_name in ['H1','L1']:\n    ifo=bilby.gw.detector.get_empty_interferometer(ifo_name)\n    ifo.set_strain_data_from_frame_file('challenge3.gwf',sampling_rate, duration, start_time=start_time ,channel=ifo_name+':CHALLENGE3')\n    \n    ifo.power_spectral_density = bilby.gw.detector.PowerSpectralDensity.from_frame_file('challenge3.gwf', psd_start_time ,\n                                                                                        psd_duration=psd_duration, fft_length=duration ,\n                                                                                        sampling_frequency=sampling_rate, roll_off= ifo.strain_data.roll_off,\n                                                                                        channel=ifo_name+':CHALLENGE3')\n    \n    #gwpy_strain = TimeSeries.read('challenge3.gwf', channel=ifo_name+\":CHALLENGE3\")\n    #sampling_rate_raw=1/gwpy_strain.dt\n    #segment = (int((start_time-psd_duration)*sampling_rate_raw.value), int((start_time)*sampling_rate_raw.value))\n    #ifo_psd_data = gwpy_strain[segment[0]:segment[1]]\n    \n    #psd_alpha = 2 * ifo.strain_data.roll_off / duration\n    #ifo_psd = ifo_psd_data.psd(fftlength=duration, overlap=0, window=(\"tukey\", psd_alpha), method=\"median\")\n    #ifo.power_spectral_density = bilby.gw.detector.PowerSpectralDensity(frequency_array=ifo_psd.frequencies.value, psd_array=ifo_psd.value)\n    \n    interferometers.append(ifo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note: In order to generate frequency_domain_strain of the same length as frequency_mask, sampling rate needs to be high enough","metadata":{}},{"cell_type":"code","source":"print(len(interferometers[0].strain_data.frequency_mask))\nprint(len(interferometers[0].strain_data.frequency_domain_strain))\nprint(len(interferometers[0].power_spectral_density.frequency_array))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nidxs = interferometers[0].strain_data.frequency_mask  # This is a boolean mask of the frequencies which we'll use in the analysis\nax.loglog(interferometers[0].strain_data.frequency_array[idxs],\n          np.abs(interferometers[0].strain_data.frequency_domain_strain[idxs]))\nax.loglog(interferometers[0].power_spectral_density.frequency_array[idxs],\n          interferometers[0].power_spectral_density.asd_array[idxs])\nax.set_xlabel(\"Frequency [Hz]\")\nax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interferometers[0].maximum_frequency = 1024\ninterferometers[1].maximum_frequency = 1024","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a prior\n\nHere, we create a prior fixing everything except the chirp mass, mass ratio, phase and geocent_time parameters to fixed values. The first two were described above. The second two give the phase of the system and the time at which it merges.","metadata":{}},{"cell_type":"code","source":"prior = bilby.core.prior.PriorDict()\nprior['chirp_mass'] = Uniform(name='chirp_mass', minimum=mass_from_mf_min,maximum=mass_from_mf_max)\nprior['mass_ratio'] = 1.0\nprior['phase'] = Uniform(name=\"phase\", minimum=0, maximum=2*np.pi)\nprior['geocent_time'] = Uniform(name=\"geocent_time\", minimum=start_time, maximum=start_time+duration)\nprior['a_1'] =  0.0\nprior['a_2'] =  0.0\nprior['tilt_1'] =  0.0\nprior['tilt_2'] =  0.0\nprior['phi_12'] =  0.0\nprior['phi_jl'] =  0.0\nprior['dec'] =  -1.2232\nprior['ra'] =  2.19432\nprior['theta_jn'] =  1.89694\nprior['psi'] =  0.532268\nprior['luminosity_distance'] = 412.066\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a likelihood\n\nFor Bayesian inference, we need to evaluate the likelihood. In Bilby, we create a likelihood object. This is the communication interface between the sampling part of Bilby and the data. Explicitly, when Bilby is sampling it only uses the `parameters` and `log_likelihood()` of the likelihood object. This means the likelihood can be arbitrarily complicated and the sampling part of Bilby won't mind a bit!\n\nLet's create a `GravitationalWaveTransient`, a special inbuilt method carefully designed to wrap up evaluating the likelihood of a waveform model in some data.","metadata":{}},{"cell_type":"code","source":"# First, put our \"data\" created above into a list of interferometers (the order is arbitrary)\n#interferometers = [H1, L1]\n\n# Next create a dictionary of arguments which we pass into the LALSimulation waveform - we specify the waveform approximant here\nwaveform_arguments = dict(\n    waveform_approximant='IMRPhenomPv2', reference_frequency=100., catch_waveform_errors=True)\n\n# Next, create a waveform_generator object. This wraps up some of the jobs of converting between parameters etc\nwaveform_generator = bilby.gw.WaveformGenerator(\n    frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole,\n    waveform_arguments=waveform_arguments,\n    parameter_conversion=convert_to_lal_binary_black_hole_parameters)\n\n# Finally, create our likelihood, passing in what is needed to get going\nlikelihood = bilby.gw.likelihood.GravitationalWaveTransient(\n    interferometers, waveform_generator, priors=prior,\n    time_marginalization=True, phase_marginalization=True, distance_marginalization=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we also specify `time_marginalization=True` and `phase_marginalization=True`. This is a trick often used in Bayesian inference. We analytically marginalize (integrate) over the time/phase of the system while sampling, effectively reducing the parameter space and making it easier to sample. Bilby will then figure out (after the sampling) posteriors for these marginalized parameters. For an introduction to this topic, see [Thrane & Talbot (2019)](https://arxiv.org/abs/1809.02293).","metadata":{}},{"cell_type":"markdown","source":"### Run the analysis","metadata":{}},{"cell_type":"markdown","source":"Now that the prior is set-up and the likelihood is set-up (with the data and the signal mode), we can run the sampler to get the posterior result. This function takes the likelihood and prior along with some options for how to do the sampling and how to save the data.","metadata":{"Collapsed":"false","colab_type":"text","id":"LCfygeVyygvM"}},{"cell_type":"code","source":"result_short = bilby.run_sampler(\n    likelihood, prior, sampler='dynesty', outdir='short', label=label_mass_from_mf,\n    conversion_function=bilby.gw.conversion.generate_all_bbh_parameters,\n    #sample=\"unif\",\n    sample=\"rwalk_dynesty\",\n    nlive=500, dlogz=3  # <- Arguments are used to make things fast - not recommended for general use\n)","metadata":{"Collapsed":"false","colab":{"base_uri":"https://localhost:8080/","height":627},"colab_type":"code","id":"HHS9JSX3ygvN","outputId":"69e07ce1-f118-4378-c750-5ec65d43e0db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at the outputs","metadata":{}},{"cell_type":"markdown","source":"The `run_sampler` returned `result_short` - this is a Bilby result object. The posterior samples are stored in a [pandas data frame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) (think of this like a spreadsheet); let's take a look at it","metadata":{"Collapsed":"false","colab_type":"text","id":"wKR045TIygvT"}},{"cell_type":"code","source":"result_short.posterior","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can pull out specific parameters that we are interested in","metadata":{}},{"cell_type":"code","source":"Mc = result_short.posterior[\"chirp_mass\"].values\nm1 = Mc/2**(1.0/5.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then get some useful quantities such as the 90\\% credible interval","metadata":{}},{"cell_type":"code","source":"lower_bound = np.quantile(m1, 0.05)\nupper_bound = np.quantile(m1, 0.95)\nmedian = np.quantile(m1, 0.5)\nprint(\"m1 = {} with a 90% C.I = {} -> {}\".format(median, lower_bound, upper_bound))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then plot the chirp mass in a histogram adding a region to indicate the 90\\% C.I.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.hist(result_short.posterior[\"chirp_mass\"]/2**(1.0/5.0), bins=20)\nax.axvspan(lower_bound, upper_bound, color='C1', alpha=0.4)\nax.axvline(median, color='C1')\nax.set_xlabel(\"component mass\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}